{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WcU4PhUZ0KeCFqUE8dKvEibMyrbOfEQq",
      "authorship_tag": "ABX9TyOOMIZpD+SYIj8zsaXzq9sM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hpzhang94/289g-project/blob/main/BiLSTM_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48LTaMV9V-rk",
        "outputId": "d53b09e9-e147-42a1-a84d-ba78f6630b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "def getR8():\n",
        "  r8train = pandas.read_csv('/content/drive/MyDrive/Colab Files/R8-lstm/train.txt', sep='\\t', names=['label', 'sentence'])\n",
        "  r8test = pandas.read_csv('/content/drive/MyDrive/Colab Files/R8-lstm/test.txt', sep='\\t', names=['label', 'sentence'])\n",
        "  return r8train, r8test\n",
        "\n",
        "def getMovieReview():\n",
        "  trainX = pandas.read_csv('/content/drive/MyDrive/Colab Files/MovieReview/text_train.txt', sep='\\t', names=['sentence'])\n",
        "  trainY = pandas.read_csv('/content/drive/MyDrive/Colab Files/MovieReview/label_train.txt', sep='\\t', names=['label'])\n",
        "  train = pandas.concat([trainX, trainY], axis=1)\n",
        "  testX = pandas.read_csv('/content/drive/MyDrive/Colab Files/MovieReview/text_test.txt', sep='\\t', names=['sentence'])\n",
        "  testY = pandas.read_csv('/content/drive/MyDrive/Colab Files/MovieReview/label_test.txt', sep='\\t', names=['label'])\n",
        "  test = pandas.concat([testX, testY], axis=1)\n",
        "  return train, test\n",
        "\n",
        "r8train, r8test = getR8()\n",
        "mr_train, mr_test = getMovieReview()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_len_stats(df):\n",
        "  df['length'] = df['sentence'].str.split().str.len()\n",
        "  print(df['length'].describe())\n",
        "get_len_stats(mr_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqP4jUuWsR30",
        "outputId": "85eec0dc-fd6d-4f8a-ed4c-dfccf5a61310"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    7108.000000\n",
            "mean       21.000563\n",
            "std         9.396583\n",
            "min         1.000000\n",
            "25%        14.000000\n",
            "50%        20.000000\n",
            "75%        27.000000\n",
            "max        59.000000\n",
            "Name: length, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(df):\n",
        "  counts = 0\n",
        "  vocab = set()\n",
        "  for index, row in df.iterrows():\n",
        "    vocab.update(tokenizer(row['sentence']))\n",
        "  return vocab\n",
        "\n",
        "def load_glove_vectors():\n",
        "  return GloVe(name='840B', dim=300)\n",
        "\n",
        "def get_embedding_matrix(pretrained, vocab, emb_size=300):\n",
        "  \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
        "  vocab_size = len(vocab) + 2\n",
        "  vocab_to_idx = {}\n",
        "  new_vocab = [\"\", \"UNK\"]\n",
        "  W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
        "  W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
        "  W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
        "  vocab_to_idx[\"UNK\"] = 1\n",
        "  i = 2\n",
        "  for word in vocab:\n",
        "    W[i] = pretrained.get_vecs_by_tokens([word], lower_case_backup=True)[0]\n",
        "    vocab_to_idx[word] = i\n",
        "    new_vocab.append(word)\n",
        "    i += 1   \n",
        "  return W, np.array(new_vocab), vocab_to_idx\n",
        "\n",
        "def encode_sentence(text, vocab_to_idx):\n",
        "  tokens = tokenizer(text)\n",
        "  actual_encoding = np.array([vocab_to_idx.get(word, vocab_to_idx[\"UNK\"]) for word in tokens])\n",
        "  return actual_encoding\n",
        "\n",
        "def df_mapping(df, vocab_to_idx, label_to_idx=None):\n",
        "  ## convert sentence to list of indices\n",
        "  df['encoding'] = df['sentence'].apply(lambda x: encode_sentence(x, vocab_to_idx))\n",
        "  if label_to_idx:\n",
        "    df['Y'] = df['label'].map(label_to_idx)\n",
        "  else:\n",
        "    df['Y'] = df['label']\n",
        "  return df\n",
        "\n",
        "def get_label_mapping(df):\n",
        "  labels = df['label'].unique()\n",
        "  label_to_idx = {labels[i]:i for i in range(len(labels))}\n",
        "  idx_to_label = {i:labels[i] for i in range(len(labels))}\n",
        "  return label_to_idx, idx_to_label\n",
        "\n",
        "# pretrained = load_glove_vectors()\n",
        "\n",
        "# for R8\n",
        "# vocab = get_vocab(r8train)\n",
        "# print(f'Vocab size is {len(vocab)}')\n",
        "# W, vocab2, vocab_to_idx = get_embedding_matrix(pretrained, vocab)\n",
        "# label_to_idx, idx_to_label = get_label_mapping(r8train)\n",
        "# train_df = df_mapping(r8train, vocab_to_idx, label_to_idx)\n",
        "# test_df = df_mapping(r8test, vocab_to_idx, label_to_idx)\n",
        "\n",
        "# for MR\n",
        "vocab = get_vocab(mr_train)\n",
        "print(f'Vocab size is {len(vocab)}')\n",
        "W, vocab2, vocab_to_idx = get_embedding_matrix(pretrained, vocab)\n",
        "label_to_idx, idx_to_label = get_label_mapping(mr_train)\n",
        "train_df = df_mapping(mr_train, vocab_to_idx)\n",
        "test_df = df_mapping(mr_test, vocab_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afzqbz-WoDgj",
        "outputId": "82f7a9dd-0641-4d3d-e404-4d251c5cefb8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size is 16615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch, pad_index=0):\n",
        "    label_list, seq_list, len_list = [], [], []\n",
        "    for (_seq, _label, _len) in batch:\n",
        "      label_list.append(_label)\n",
        "      seq_list.append(torch.tensor(_seq))\n",
        "      len_list.append(_len)\n",
        "    return pad_sequence(seq_list, batch_first=True, padding_value=pad_index), torch.tensor(label_list), torch.tensor(len_list)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = X\n",
        "    self.y = Y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return torch.from_numpy(self.X[idx].astype(np.int32)), self.y[idx], len(self.X[idx])\n",
        "\n",
        "trainX = list(train_df['encoding'])\n",
        "trainY = list(train_df['Y'])\n",
        "testX = list(test_df['encoding'])\n",
        "testY = list(test_df['Y'])\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
        "train_ds = MyDataset(trainX, trainY)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, collate_fn=collate, shuffle=True)\n",
        "test_ds = MyDataset(testX, testY)\n",
        "test_dl = DataLoader(test_ds, batch_size=64, collate_fn=collate, shuffle=False)"
      ],
      "metadata": {
        "id": "dCk6JD8U7Or4"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, pretrained, label_size, dimension=128):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
        "    self.embedding.weight.data.copy_(torch.from_numpy(pretrained))\n",
        "    self.embedding.weight.requires_grd = False ## freeze weights\n",
        "    self.dimension = dimension\n",
        "    self.lstm = nn.LSTM(input_size=300, hidden_size=dimension, num_layers=1, batch_first=True, bidirectional=True)\n",
        "    self.drop = nn.Dropout(p=0.2)\n",
        "    self.fc = nn.Linear(2*dimension, label_size)\n",
        "  \n",
        "  def forward(self, text, text_len):\n",
        "    text_emb = self.embedding(text)\n",
        "    packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    packed_output, _ = self.lstm(packed_input)\n",
        "    output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "    out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
        "    out_reverse = output[:, 0, self.dimension:]\n",
        "    out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "    text_fea = self.drop(out_reduced)\n",
        "\n",
        "    text_fea = self.fc(text_fea)\n",
        "    text_fea = torch.squeeze(text_fea, 1)\n",
        "    text_out = torch.sigmoid(text_fea)\n",
        "    return text_out\n",
        "  \n",
        "model = LSTM(len(vocab2), W, len(label_to_idx)).to(device)"
      ],
      "metadata": {
        "id": "PMZ8TZ-NYkvt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n"
      ],
      "metadata": {
        "id": "dPa3eBSEj0TR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          criterion = nn.CrossEntropyLoss(),\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_dl) // 2,\n",
        "          file_path = '/content/drive/MyDrive/Colab Files/MovieReview',\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    # print(valid_loader)\n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for x, y, length in train_loader:   \n",
        "            x = x.long().to(device)\n",
        "            y = y.long().to(device)\n",
        "            length = length.long().to(device)\n",
        "            output = model(x, length)\n",
        "\n",
        "            loss = criterion(output, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "                  # validation loop\n",
        "                  for test_x, test_y, test_len in valid_loader:\n",
        "                      test_x = test_x.long().to(device)\n",
        "                      test_y = test_y.long().to(device)\n",
        "                      test_len = test_len.long().to(device)\n",
        "                      test_output = model(test_x, test_len)\n",
        "\n",
        "                      loss = criterion(test_output, test_y)\n",
        "                      valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
        "                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train(model, optimizer, train_dl, test_dl, num_epochs=30)\n",
        "y_pred = evaluate(model, test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzLsDYaBj2ab",
        "outputId": "5c5d7844-11e2-42f9-f935-e039ae1d5afa"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [56/3360], Train Loss: 0.3203, Valid Loss: 0.5554\n",
            "Model saved to ==> /content/drive/MyDrive/Colab Files/MovieReview/model.pt\n",
            "Model saved to ==> /content/drive/MyDrive/Colab Files/MovieReview/metrics.pt\n",
            "Epoch [1/30], Step [112/3360], Train Loss: 0.3208, Valid Loss: 0.5444\n",
            "Model saved to ==> /content/drive/MyDrive/Colab Files/MovieReview/model.pt\n",
            "Model saved to ==> /content/drive/MyDrive/Colab Files/MovieReview/metrics.pt\n",
            "Epoch [2/30], Step [168/3360], Train Loss: 0.3225, Valid Loss: 0.5624\n",
            "Epoch [2/30], Step [224/3360], Train Loss: 0.3209, Valid Loss: 0.5542\n",
            "Epoch [3/30], Step [280/3360], Train Loss: 0.3230, Valid Loss: 0.5513\n",
            "Epoch [3/30], Step [336/3360], Train Loss: 0.3207, Valid Loss: 0.5438\n",
            "Model saved to ==> /content/drive/MyDrive/Colab Files/MovieReview/model.pt\n",
            "Model saved to ==> /content/drive/MyDrive/Colab Files/MovieReview/metrics.pt\n",
            "Epoch [4/30], Step [392/3360], Train Loss: 0.3224, Valid Loss: 0.5615\n",
            "Epoch [4/30], Step [448/3360], Train Loss: 0.3211, Valid Loss: 0.5460\n",
            "Epoch [5/30], Step [504/3360], Train Loss: 0.3207, Valid Loss: 0.5483\n",
            "Epoch [5/30], Step [560/3360], Train Loss: 0.3218, Valid Loss: 0.5492\n",
            "Epoch [6/30], Step [616/3360], Train Loss: 0.3189, Valid Loss: 0.5486\n",
            "Epoch [6/30], Step [672/3360], Train Loss: 0.3213, Valid Loss: 0.5479\n",
            "Epoch [7/30], Step [728/3360], Train Loss: 0.3204, Valid Loss: 0.5522\n",
            "Epoch [7/30], Step [784/3360], Train Loss: 0.3191, Valid Loss: 0.5494\n",
            "Epoch [8/30], Step [840/3360], Train Loss: 0.3191, Valid Loss: 0.5490\n",
            "Epoch [8/30], Step [896/3360], Train Loss: 0.3202, Valid Loss: 0.5465\n",
            "Epoch [9/30], Step [952/3360], Train Loss: 0.3186, Valid Loss: 0.5466\n",
            "Epoch [9/30], Step [1008/3360], Train Loss: 0.3205, Valid Loss: 0.5463\n",
            "Epoch [10/30], Step [1064/3360], Train Loss: 0.3186, Valid Loss: 0.5462\n",
            "Epoch [10/30], Step [1120/3360], Train Loss: 0.3205, Valid Loss: 0.5462\n",
            "Epoch [11/30], Step [1176/3360], Train Loss: 0.3183, Valid Loss: 0.5463\n",
            "Epoch [11/30], Step [1232/3360], Train Loss: 0.3208, Valid Loss: 0.5463\n",
            "Epoch [12/30], Step [1288/3360], Train Loss: 0.3188, Valid Loss: 0.5461\n",
            "Epoch [12/30], Step [1344/3360], Train Loss: 0.3200, Valid Loss: 0.5461\n",
            "Epoch [13/30], Step [1400/3360], Train Loss: 0.3180, Valid Loss: 0.5471\n",
            "Epoch [13/30], Step [1456/3360], Train Loss: 0.3205, Valid Loss: 0.5526\n",
            "Epoch [14/30], Step [1512/3360], Train Loss: 0.3194, Valid Loss: 0.5522\n",
            "Epoch [14/30], Step [1568/3360], Train Loss: 0.3191, Valid Loss: 0.5516\n",
            "Epoch [15/30], Step [1624/3360], Train Loss: 0.3208, Valid Loss: 0.5513\n",
            "Epoch [15/30], Step [1680/3360], Train Loss: 0.3177, Valid Loss: 0.5512\n",
            "Epoch [16/30], Step [1736/3360], Train Loss: 0.3183, Valid Loss: 0.5508\n",
            "Epoch [16/30], Step [1792/3360], Train Loss: 0.3207, Valid Loss: 0.5534\n",
            "Epoch [17/30], Step [1848/3360], Train Loss: 0.3194, Valid Loss: 0.5551\n",
            "Epoch [17/30], Step [1904/3360], Train Loss: 0.3191, Valid Loss: 0.5560\n",
            "Epoch [18/30], Step [1960/3360], Train Loss: 0.3188, Valid Loss: 0.5569\n",
            "Epoch [18/30], Step [2016/3360], Train Loss: 0.3194, Valid Loss: 0.5569\n",
            "Epoch [19/30], Step [2072/3360], Train Loss: 0.3186, Valid Loss: 0.5569\n",
            "Epoch [19/30], Step [2128/3360], Train Loss: 0.3191, Valid Loss: 0.5582\n",
            "Epoch [20/30], Step [2184/3360], Train Loss: 0.3200, Valid Loss: 0.5573\n",
            "Epoch [20/30], Step [2240/3360], Train Loss: 0.3177, Valid Loss: 0.5584\n",
            "Epoch [21/30], Step [2296/3360], Train Loss: 0.3194, Valid Loss: 0.5580\n",
            "Epoch [21/30], Step [2352/3360], Train Loss: 0.3180, Valid Loss: 0.5585\n",
            "Epoch [22/30], Step [2408/3360], Train Loss: 0.3186, Valid Loss: 0.5589\n",
            "Epoch [22/30], Step [2464/3360], Train Loss: 0.3185, Valid Loss: 0.5576\n",
            "Epoch [23/30], Step [2520/3360], Train Loss: 0.3194, Valid Loss: 0.5551\n",
            "Epoch [23/30], Step [2576/3360], Train Loss: 0.3175, Valid Loss: 0.5555\n",
            "Epoch [24/30], Step [2632/3360], Train Loss: 0.3191, Valid Loss: 0.5558\n",
            "Epoch [24/30], Step [2688/3360], Train Loss: 0.3177, Valid Loss: 0.5559\n",
            "Epoch [25/30], Step [2744/3360], Train Loss: 0.3186, Valid Loss: 0.5558\n",
            "Epoch [25/30], Step [2800/3360], Train Loss: 0.3183, Valid Loss: 0.5560\n",
            "Epoch [26/30], Step [2856/3360], Train Loss: 0.3183, Valid Loss: 0.5560\n",
            "Epoch [26/30], Step [2912/3360], Train Loss: 0.3186, Valid Loss: 0.5560\n",
            "Epoch [27/30], Step [2968/3360], Train Loss: 0.3202, Valid Loss: 0.5560\n",
            "Epoch [27/30], Step [3024/3360], Train Loss: 0.3166, Valid Loss: 0.5561\n",
            "Epoch [28/30], Step [3080/3360], Train Loss: 0.3177, Valid Loss: 0.5561\n",
            "Epoch [28/30], Step [3136/3360], Train Loss: 0.3191, Valid Loss: 0.5561\n",
            "Epoch [29/30], Step [3192/3360], Train Loss: 0.3188, Valid Loss: 0.5562\n",
            "Epoch [29/30], Step [3248/3360], Train Loss: 0.3180, Valid Loss: 0.5561\n",
            "Epoch [30/30], Step [3304/3360], Train Loss: 0.3183, Valid Loss: 0.5561\n",
            "Epoch [30/30], Step [3360/3360], Train Loss: 0.3186, Valid Loss: 0.5561\n",
            "Model saved to ==> /content/drive/MyDrive/Colab Files/MovieReview/metrics.pt\n",
            "Finished Training!\n",
            "Accuracy is 0.7523916713562183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for test_x, test_y, test_len in test_loader:\n",
        "        test_x = test_x.long().to(device)\n",
        "        test_y = test_y.long().to(device)\n",
        "        test_len = test_len.long().to(device)\n",
        "        test_output = model(test_x, test_len)\n",
        "        output = torch.argmax(test_output, dim=1)\n",
        "        y_pred.extend(output.tolist())\n",
        "        y_true.extend(test_y.tolist())\n",
        "    \n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy is {accuracy}')\n",
        "    return y_pred\n",
        "    \n",
        "    \n",
        "best_model = LSTM(len(vocab2), W, len(label_to_idx)).to(device)\n",
        "# optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
        "\n",
        "load_checkpoint('/content/drive/MyDrive/Colab Files/MovieReview/model.pt', best_model, optimizer)\n",
        "pred = pandas.DataFrame({'pred': evaluate(best_model, test_dl)})\n",
        "pred.to_csv('/content/drive/MyDrive/Colab Files/MovieReview/mr-bilstm-pred.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPnP7WjTVPBa",
        "outputId": "d6b85254-0531-4f62-c1dc-3f439e0aa3c3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /content/drive/MyDrive/Colab Files/MovieReview/model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is 0.7664603263927968\n"
          ]
        }
      ]
    }
  ]
}