{"cells":[{"cell_type":"markdown","metadata":{"id":"3l6d1O7gbEOY"},"source":["# ECS289G Project: CNN for Text Classification using PyTorch\n","\n","* A PyTorch implementation for CNN Text Classification based on [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) (Kim, 2014).\n","\n","* Skeleton of this notebook is based on this post: https://chriskhanhtran.github.io/posts/cnn-sentence-classification/\n","    * We are using Glove 6B 300d for pretrained embeddings, tuned hyperparameters, and extend the datasets to both MR and R8\n","    * Datasets and the split strategy is from https://github.com/yao8839836/text_gcn/tree/master/data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:55:11.970828Z","iopub.status.busy":"2022-11-20T19:55:11.970429Z","iopub.status.idle":"2022-11-20T19:55:26.629538Z","shell.execute_reply":"2022-11-20T19:55:26.628549Z","shell.execute_reply.started":"2022-11-20T19:55:11.970794Z"},"id":"6mMXqnuSa-df","outputId":"64a6143c-45de-45e8-8e8a-268b83f043fb","trusted":true},"outputs":[],"source":["import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download(\"all\")\n","import matplotlib.pyplot as plt\n","import torch\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"FV5yjMaIbLuR"},"source":["## Data Preprocessing\n","\n","### Datasets\n","Datasets that we will be using for our project are:\n","* Movie Review(MR): http://www.cs.cornell.edu/people/pabo/movie-review-data/\n","* R8: https://www.cs.umb.edu/Ëœsmimarog/textmining/datasets/\n","* They can also be downloaded [here](https://github.com/yao8839836/text_gcn/tree/master/data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:55:26.632420Z","iopub.status.busy":"2022-11-20T19:55:26.631822Z","iopub.status.idle":"2022-11-20T19:55:36.637915Z","shell.execute_reply":"2022-11-20T19:55:36.636630Z","shell.execute_reply.started":"2022-11-20T19:55:26.632379Z"},"id":"vQY6VnMHbP4P","outputId":"6447ebea-1322-44ab-a22e-29e61a6520ea","trusted":true},"outputs":[],"source":["# Download datasets\n","!rm -rf ./data\n","!wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/text_train.txt -P ./data/mr/\n","!wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/text_test.txt -P ./data/mr/\n","!wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/label_train.txt -P ./data/mr/\n","!wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/label_test.txt -P ./data/mr/\n","!wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/R8/train.txt -P ./data/R8/\n","!wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/R8/test.txt -P ./data/R8/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:55:36.640990Z","iopub.status.busy":"2022-11-20T19:55:36.639943Z","iopub.status.idle":"2022-11-20T19:55:36.816490Z","shell.execute_reply":"2022-11-20T19:55:36.815465Z","shell.execute_reply.started":"2022-11-20T19:55:36.640947Z"},"id":"YYpB4aU3ettv","trusted":true},"outputs":[],"source":["# Preprocessing into \"list of texts\" and \"list of labels\"\n","def load_mr(filename):\n","    with open(filename, 'rb') as f:\n","        texts = []\n","        for line in f:\n","            item = line.decode(errors='ignore').lower().strip()\n","            if item == '0' or item == '1':\n","                item = int(item)\n","            texts.append(item)\n","\n","    return np.array(texts)\n","\n","r8_dict = {\n","    \"acq\": 0,\n","    \"crude\": 1,\t\n","    \"earn\": 2,\t\n","    \"grain\": 3,\n","    \"interest\": 4,\t\n","    \"money-fx\": 5,\n","    \"ship\": 6,\n","    \"trade\": 7\n","}\n","def load_r8(filename):\n","    with open(filename, 'rb') as f:\n","        texts = []\n","        labels = []\n","        for line in f:\n","            line = line.decode(errors='ignore').split(\"\\t\")\n","            text, label = line[1], line[0]\n","            texts.append(text)\n","            labels.append(r8_dict[label])\n","\n","    return np.array(texts), np.array(labels)\n","\n","mr_train_texts = load_mr('./data/mr/text_train.txt')\n","mr_test_texts = load_mr('./data/mr/text_test.txt')\n","mr_train_labels = load_mr('./data/mr/label_train.txt')\n","mr_test_labels = load_mr('./data/mr/label_test.txt')\n","r8_train_texts, r8_train_labels = load_r8('./data/R8/train.txt') \n","r8_test_texts, r8_test_labels = load_r8('./data/R8/test.txt')"]},{"cell_type":"markdown","metadata":{"id":"ebKqyWK1bTRr"},"source":["## Pretrained Embedding and Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:55:36.819697Z","iopub.status.busy":"2022-11-20T19:55:36.819211Z","iopub.status.idle":"2022-11-20T19:56:35.895833Z","shell.execute_reply":"2022-11-20T19:56:35.894652Z","shell.execute_reply.started":"2022-11-20T19:55:36.819656Z"},"id":"0BveqiEDbZHG","outputId":"018bec4e-d933-4953-e0a6-60744e789c20","trusted":true},"outputs":[],"source":["# Download Glove Embeddings\n","URL = \"https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\"\n","FILE = \"Glove\"\n","\n","if os.path.isdir(FILE):\n","    print(\"Glove exists.\")\n","else:\n","    !wget -P $FILE $URL\n","    !unzip $FILE/glove.6B.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:35.898461Z","iopub.status.busy":"2022-11-20T19:56:35.897730Z","iopub.status.idle":"2022-11-20T19:56:35.909203Z","shell.execute_reply":"2022-11-20T19:56:35.908147Z","shell.execute_reply.started":"2022-11-20T19:56:35.898420Z"},"id":"oBalZE1ygjBo","trusted":true},"outputs":[],"source":["# tokenization\n","from nltk.tokenize import word_tokenize\n","from collections import defaultdict\n","\n","def tokenize(texts):\n","    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n","    \n","    Args:\n","        texts (List[str]): List of text data\n","    \n","    Returns:\n","        tokenized_texts (List[List[str]]): List of list of tokens\n","        word2idx (Dict): Vocabulary built from the corpus\n","        max_len (int): Maximum sentence length\n","    \"\"\"\n","\n","    max_len = 0\n","    tokenized_texts = []\n","    word2idx = {}\n","\n","    # Add <pad> and <unk> tokens to the vocabulary\n","    word2idx['<pad>'] = 0\n","    word2idx['<unk>'] = 1\n","\n","    # Building our vocab from the corpus starting from index 2\n","    idx = 2\n","    for sent in texts:\n","        tokenized_sent = word_tokenize(sent)\n","\n","        # Add `tokenized_sent` to `tokenized_texts`\n","        tokenized_texts.append(tokenized_sent)\n","\n","        # Add new token to `word2idx`\n","        for token in tokenized_sent:\n","            if token not in word2idx:\n","                word2idx[token] = idx\n","                idx += 1\n","\n","        # Update `max_len`\n","        max_len = max(max_len, len(tokenized_sent))\n","\n","    return tokenized_texts, word2idx, max_len\n","\n","def encode(tokenized_texts, word2idx, max_len):\n","    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n","    their index in the vocabulary.\n","\n","    Returns:\n","        input_ids (np.array): Array of token indexes in the vocabulary with\n","            shape (N, max_len). It will the input of our CNN model.\n","    \"\"\"\n","\n","    input_ids = []\n","    for tokenized_sent in tokenized_texts:\n","        # Pad sentences to max_len\n","        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n","\n","        # Encode tokens to input_ids\n","        input_id = [word2idx.get(token) for token in tokenized_sent]\n","        input_ids.append(input_id)\n","    \n","    return np.array(input_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:35.911046Z","iopub.status.busy":"2022-11-20T19:56:35.910589Z","iopub.status.idle":"2022-11-20T19:56:37.223070Z","shell.execute_reply":"2022-11-20T19:56:37.221680Z","shell.execute_reply.started":"2022-11-20T19:56:35.911004Z"},"id":"MXiDltX5w8XW","trusted":true},"outputs":[],"source":["# load embeddings\n","from tqdm import tqdm_notebook\n","\n","def load_pretrained_vectors(word2idx, fname):\n","    \"\"\"Load pretrained vectors and create embedding layers.\n","    \n","    Args:\n","        word2idx (Dict): Vocabulary built from the corpus\n","        fname (str): Path to pretrained vector file\n","\n","    Returns:\n","        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n","            the size of word2idx and d is embedding dimension\n","    \"\"\"\n","\n","    print(\"Loading pretrained vectors...\")\n","    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    # n, d = map(int, fin.readline().split())\n","    d=300\n","\n","    # Initilize random embeddings\n","    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n","    embeddings[word2idx['<pad>']] = np.zeros((d,))\n","\n","    # Load pretrained vectors\n","    count = 0\n","    for line in tqdm_notebook(fin):\n","        tokens = line.rstrip().split(' ')\n","        word = tokens[0]\n","        if word in word2idx:\n","            count += 1\n","            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n","\n","    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n","\n","    return embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:37.225549Z","iopub.status.busy":"2022-11-20T19:56:37.224849Z","iopub.status.idle":"2022-11-20T19:56:52.648411Z","shell.execute_reply":"2022-11-20T19:56:52.647100Z","shell.execute_reply.started":"2022-11-20T19:56:37.225509Z"},"id":"ZgmSm_SDxDbp","outputId":"979befbb-22f5-4258-8d8d-af1fa623a95a","trusted":true},"outputs":[],"source":["train_texts = mr_train_texts\n","test_texts = mr_test_texts\n","# Tokenize, build vocabulary, encode tokens\n","print(\"Tokenizing...\\n\")\n","tokenized_texts_train, word2idx, max_len = tokenize(train_texts)\n","tokenized_texts_test, word2idx, max_len = tokenize(test_texts)\n","tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n","input_ids_train = encode(tokenized_texts_train, word2idx, max_len)\n","input_ids_test = encode(tokenized_texts_test, word2idx, max_len)\n","\n","\n","# Load pretrained vectors\n","# tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n","embeddings = load_pretrained_vectors(word2idx, \"glove.6B.300d.txt\")\n","embeddings = torch.tensor(embeddings)"]},{"cell_type":"markdown","metadata":{"id":"nfizta7YxVtL"},"source":["## Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:52.650671Z","iopub.status.busy":"2022-11-20T19:56:52.650260Z","iopub.status.idle":"2022-11-20T19:56:52.658535Z","shell.execute_reply":"2022-11-20T19:56:52.657412Z","shell.execute_reply.started":"2022-11-20T19:56:52.650629Z"},"id":"naCwskAyxYk_","trusted":true},"outputs":[],"source":["from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n","                              SequentialSampler)\n","\n","def data_loader(train_inputs, test_inputs, train_labels, test_labels,\n","                batch_size=50):\n","    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n","    DataLoader.\n","    \"\"\"\n","\n","    # Convert data type to torch.Tensor\n","    train_inputs, test_inputs, train_labels, test_labels =\\\n","    tuple(torch.tensor(data) for data in\n","          [train_inputs, test_inputs, train_labels, test_labels])\n","\n","    # Specify batch_size\n","    batch_size = batch_size\n","\n","    # Create DataLoader for training data\n","    train_data = TensorDataset(train_inputs, train_labels)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    # Create DataLoader for validation data\n","    test_data = TensorDataset(test_inputs, test_labels)\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","\n","    return train_dataloader, test_dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:52.660886Z","iopub.status.busy":"2022-11-20T19:56:52.660212Z","iopub.status.idle":"2022-11-20T19:56:52.677979Z","shell.execute_reply":"2022-11-20T19:56:52.677021Z","shell.execute_reply.started":"2022-11-20T19:56:52.660836Z"},"id":"QAQn1Sx4x33y","trusted":true},"outputs":[],"source":["# Load data to PyTorch DataLoader\n","train_inputs = input_ids_train\n","test_inputs = input_ids_test\n","train_labels = mr_train_labels\n","test_labels = mr_test_labels\n","train_dataloader, test_dataloader = \\\n","data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)"]},{"cell_type":"markdown","metadata":{"id":"zEiZzgf6bZ4C"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:52.682630Z","iopub.status.busy":"2022-11-20T19:56:52.682373Z","iopub.status.idle":"2022-11-20T19:56:52.746985Z","shell.execute_reply":"2022-11-20T19:56:52.745864Z","shell.execute_reply.started":"2022-11-20T19:56:52.682607Z"},"id":"o56wzakXbcao","outputId":"5d6e7798-467a-49fd-90ad-4d2002d70a85","trusted":true},"outputs":[],"source":["# set up GPU\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:52.749072Z","iopub.status.busy":"2022-11-20T19:56:52.748696Z","iopub.status.idle":"2022-11-20T19:56:52.763045Z","shell.execute_reply":"2022-11-20T19:56:52.762051Z","shell.execute_reply.started":"2022-11-20T19:56:52.749038Z"},"id":"SjTY2yHozRxA","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN_NLP(nn.Module):\n","    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n","    def __init__(self,\n","                 pretrained_embedding=None,\n","                 freeze_embedding=False,\n","                 vocab_size=None,\n","                 embed_dim=300,\n","                 filter_sizes=[3, 4, 5],\n","                 num_filters=[100, 100, 100],\n","                 num_classes=2,\n","                 dropout=0.5):\n","        \"\"\"\n","        The constructor for CNN_NLP class.\n","\n","        Args:\n","            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n","                shape (vocab_size, embed_dim)\n","            freeze_embedding (bool): Set to False to fine-tune pretraiend\n","                vectors. Default: False\n","            vocab_size (int): Need to be specified when not pretrained word\n","                embeddings are not used.\n","            embed_dim (int): Dimension of word vectors. Need to be specified\n","                when pretrained word embeddings are not used. Default: 300\n","            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n","            num_filters (List[int]): List of number of filters, has the same\n","                length as `filter_sizes`. Default: [100, 100, 100]\n","            n_classes (int): Number of classes. Default: 2\n","            dropout (float): Dropout rate. Default: 0.5\n","        \"\"\"\n","\n","        super(CNN_NLP, self).__init__()\n","        # Embedding layer\n","        if pretrained_embedding is not None:\n","            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n","            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n","                                                          freeze=freeze_embedding)\n","        else:\n","            self.embed_dim = embed_dim\n","            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                          embedding_dim=self.embed_dim,\n","                                          padding_idx=0,\n","                                          max_norm=5.0)\n","        # Conv Network\n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=self.embed_dim,\n","                      out_channels=num_filters[i],\n","                      kernel_size=filter_sizes[i])\n","            for i in range(len(filter_sizes))\n","        ])\n","        # Fully-connected layer and Dropout\n","        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, input_ids):\n","        \"\"\"Perform a forward pass through the network.\n","\n","        Args:\n","            input_ids (torch.Tensor): A tensor of token ids with shape\n","                (batch_size, max_sent_length)\n","\n","        Returns:\n","            logits (torch.Tensor): Output logits with shape (batch_size,\n","                n_classes)\n","        \"\"\"\n","\n","        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n","        x_embed = self.embedding(input_ids).float()\n","\n","        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n","        # Output shape: (b, embed_dim, max_len)\n","        x_reshaped = x_embed.permute(0, 2, 1)\n","\n","        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n","        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n","\n","        # Max pooling. Output shape: (b, num_filters[i], 1)\n","        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n","            for x_conv in x_conv_list]\n","        \n","        # Concatenate x_pool_list to feed the fully connected layer.\n","        # Output shape: (b, sum(num_filters))\n","        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n","                         dim=1)\n","        \n","        # Compute logits. Output shape: (b, n_classes)\n","        logits = self.fc(self.dropout(x_fc))\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:52.764909Z","iopub.status.busy":"2022-11-20T19:56:52.764357Z","iopub.status.idle":"2022-11-20T19:56:52.775670Z","shell.execute_reply":"2022-11-20T19:56:52.774924Z","shell.execute_reply.started":"2022-11-20T19:56:52.764872Z"},"id":"0YU6Jx-HzVn3","trusted":true},"outputs":[],"source":["import torch.optim as optim\n","\n","def initilize_model(pretrained_embedding=None,\n","                    freeze_embedding=False,\n","                    vocab_size=None,\n","                    embed_dim=300,\n","                    filter_sizes=[3, 4, 5],\n","                    num_filters=[100, 100, 100],\n","                    num_classes=2,\n","                    dropout=0.5,\n","                    learning_rate=0.01,\n","                    weight_decay=0):\n","    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n","\n","    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n","    num_filters need to be of the same length.\"\n","\n","    # Instantiate CNN model\n","    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n","                        freeze_embedding=freeze_embedding,\n","                        vocab_size=vocab_size,\n","                        embed_dim=embed_dim,\n","                        filter_sizes=filter_sizes,\n","                        num_filters=num_filters,\n","                        num_classes=num_classes,\n","                        dropout=0.5)\n","    \n","    # Send model to `device` (GPU/CPU)\n","    cnn_model.to(device)\n","\n","    # Instantiate Adadelta optimizer\n","    optimizer = optim.Adadelta(cnn_model.parameters(),\n","                               lr=learning_rate,\n","                               rho=0.95,\n","                               weight_decay=weight_decay)\n","\n","    return cnn_model, optimizer"]},{"cell_type":"markdown","metadata":{"id":"e4xSA45kbd7b"},"source":["## Training and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:52.779377Z","iopub.status.busy":"2022-11-20T19:56:52.778878Z","iopub.status.idle":"2022-11-20T19:56:52.795104Z","shell.execute_reply":"2022-11-20T19:56:52.794141Z","shell.execute_reply.started":"2022-11-20T19:56:52.779349Z"},"id":"jrP2hQmdbgwx","trusted":true},"outputs":[],"source":["import random\n","import time\n","\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\"\"\"\n","\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, optimizer, train_dataloader, test_dataloader=None, epochs=10, model_name=\"\"):\n","    \"\"\"Train the CNN model.\"\"\"\n","    \n","    # Tracking best validation accuracy\n","    best_accuracy = 0\n","    train_time = 0\n","\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test Acc':^9} | {'Elapsed':^9}\")\n","    print(\"-\"*60)\n","\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","\n","        # Tracking time and loss\n","        t0_epoch = time.time()\n","        total_loss = 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","            # Load batch to GPU\n","            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Update parameters\n","            optimizer.step()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        \n","        train_time += time.time() - t0_epoch\n","\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if test_dataloader is not None:\n","            # After the completion of each training epoch, measure the model's\n","            # performance on our validation set.\n","            test_loss, test_accuracy = evaluate(model, test_dataloader)\n","\n","            # Track the best accuracy\n","            if test_accuracy > best_accuracy:\n","                best_accuracy = test_accuracy\n","                torch.save(model, \"./models/\" + model_name + \"_best_model.pt\")\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {test_loss:^10.6f} | {test_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            \n","    print(\"\\n\")\n","    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n","    return best_accuracy, train_time\n","\n","def evaluate(model, test_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's\n","    performance on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled\n","    # during the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    test_accuracy = []\n","    test_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        test_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        test_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    test_loss = np.mean(test_loss)\n","    test_accuracy = np.mean(test_accuracy)\n","\n","    return test_loss, test_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:52.798384Z","iopub.status.busy":"2022-11-20T19:56:52.797576Z","iopub.status.idle":"2022-11-20T19:56:53.761841Z","shell.execute_reply":"2022-11-20T19:56:53.760557Z","shell.execute_reply.started":"2022-11-20T19:56:52.798348Z"},"id":"I0m1ocTtdipj","trusted":true},"outputs":[],"source":["!mkdir ./models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:53.764463Z","iopub.status.busy":"2022-11-20T19:56:53.764037Z","iopub.status.idle":"2022-11-20T19:56:53.770741Z","shell.execute_reply":"2022-11-20T19:56:53.769568Z","shell.execute_reply.started":"2022-11-20T19:56:53.764424Z"},"trusted":true},"outputs":[],"source":["# parameters count\n","def count_params(model):\n","    pytorch_total_params = sum(p.numel() for p in model.parameters())\n","    pytorch_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Total Parameters: \" + str(pytorch_total_params))\n","    print(\"Trainable Parameters: \" + str(pytorch_trainable_params))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:53.772899Z","iopub.status.busy":"2022-11-20T19:56:53.772265Z","iopub.status.idle":"2022-11-20T19:56:53.784410Z","shell.execute_reply":"2022-11-20T19:56:53.783504Z","shell.execute_reply.started":"2022-11-20T19:56:53.772863Z"},"trusted":true},"outputs":[],"source":["# cal std and mean of list\n","def mean_std(arr):\n","    arr = np.array(arr)\n","    mean = np.mean(arr)\n","    std = np.std(arr)\n","    return mean, std\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-11-20T19:54:56.426521Z","iopub.status.idle":"2022-11-20T19:54:56.428068Z","shell.execute_reply":"2022-11-20T19:54:56.427836Z","shell.execute_reply.started":"2022-11-20T19:54:56.427812Z"},"id":"qLMj1Vk2bkEg","outputId":"c99e5bc6-5e51-42ce-ebae-7975688744b1","trusted":true},"outputs":[],"source":["# need to create dir: \"./models\"\n","# CNN-rand: Word vectors are randomly initialized.\n","# set_seed(42)\n","cnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\n","                                      embed_dim=300,\n","                                      learning_rate=0.5,\n","                                      dropout=0.5,\n","                                      weight_decay=1e-3)\n","count_params(cnn_rand)\n","acc = []\n","train_t = []\n","for i in range(10):\n","    best_acc, train_time = train(cnn_rand, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"mr_cnn_rand\")\n","    acc.append(best_acc)\n","    train_t.append(train_time)\n","\n","# cal avg and std of acc and time\n","acc_mean, acc_std = mean_std(acc)\n","t_mean, t_std = mean_std(train_t)\n","print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n","print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOTxf4khzlkb","outputId":"cf2529ea-82c0-433e-ad2d-b68b49775aa2","trusted":true},"outputs":[],"source":["# CNN-static: pretrained word vectors are used and freezed during training.\n","# set_seed(42)\n","cnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n","                                        freeze_embedding=True,\n","                                        learning_rate=0.5,\n","                                        dropout=0.5,\n","                                        weight_decay=1e-3)\n","count_params(cnn_static)\n","acc = []\n","train_t = []\n","for i in range(10):\n","    best_acc, train_time = train(cnn_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"mr_cnn_static\")\n","    acc.append(best_acc)\n","    train_t.append(train_time)\n","    \n","# cal avg and std of acc and time\n","acc_mean, acc_std = mean_std(acc)\n","t_mean, t_std = mean_std(train_t)\n","print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n","print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_o_p7JwYznlC","outputId":"c81a2086-9f92-4952-b2fb-d74563b637be"},"outputs":[],"source":["# CNN-non-static: pretrained word vectors are fine-tuned during training.\n","# set_seed(42)\n","cnn_non_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n","                                            freeze_embedding=False,\n","                                            learning_rate=0.5,\n","                                            dropout=0.5,\n","                                            weight_decay=1e-3)\n","count_params(cnn_non_static)\n","acc = []\n","train_t = []\n","for i in range(10):\n","    best_acc, train_time = train(cnn_non_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"mr_cnn_non_static\")\n","    acc.append(best_acc)\n","    train_t.append(train_time)\n","\n","# cal avg and std of acc and time\n","acc_mean, acc_std = mean_std(acc)\n","t_mean, t_std = mean_std(train_t)\n","print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n","print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"]},{"cell_type":"markdown","metadata":{"id":"wtBkHiLujY1O"},"source":["## R8 Dataset Experiment"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:56:53.786386Z","iopub.status.busy":"2022-11-20T19:56:53.785883Z","iopub.status.idle":"2022-11-20T19:57:08.890319Z","shell.execute_reply":"2022-11-20T19:57:08.889260Z","shell.execute_reply.started":"2022-11-20T19:56:53.786352Z"},"id":"_NB5Pltjjdyq","outputId":"c7b6c854-f8de-4bf1-f39c-2e3be2a49ec7","trusted":true},"outputs":[],"source":["train_texts = r8_train_texts\n","test_texts = r8_test_texts\n","# Tokenize, build vocabulary, encode tokens\n","print(\"Tokenizing...\\n\")\n","tokenized_texts_train, word2idx, max_len = tokenize(train_texts)\n","tokenized_texts_test, word2idx, max_len = tokenize(test_texts)\n","tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n","input_ids_train = encode(tokenized_texts_train, word2idx, max_len)\n","input_ids_test = encode(tokenized_texts_test, word2idx, max_len)\n","\n","\n","# Load pretrained vectors\n","# tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n","embeddings = load_pretrained_vectors(word2idx, \"glove.6B.300d.txt\")\n","embeddings = torch.tensor(embeddings)\n","\n","# Load data to PyTorch DataLoader\n","train_inputs = input_ids_train\n","test_inputs = input_ids_test\n","train_labels = r8_train_labels\n","test_labels = r8_test_labels\n","train_dataloader, test_dataloader = \\\n","data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"st8iJiZ4kB5T","outputId":"7b3dfc02-b372-4efa-a0fa-d11d87a93537"},"outputs":[],"source":["# CNN-rand: Word vectors are randomly initialized.\n","# set_seed(42)\n","cnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\n","                                      embed_dim=300,\n","                                      learning_rate=0.5,\n","                                      dropout=0.5,\n","                                      num_classes=8,\n","                                      weight_decay=1e-3)\n","count_params(cnn_rand)\n","acc = []\n","train_t = []\n","for i in range(10):\n","    best_acc, train_time = train(cnn_rand, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"r8_cnn_rand\")\n","    acc.append(best_acc)\n","    train_t.append(train_time)\n","\n","# cal avg and std of acc and time\n","acc_mean, acc_std = mean_std(acc)\n","t_mean, t_std = mean_std(train_t)\n","print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n","print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:59:14.090564Z","iopub.status.busy":"2022-11-20T19:59:14.090183Z","iopub.status.idle":"2022-11-20T20:00:28.750437Z","shell.execute_reply":"2022-11-20T20:00:28.748752Z","shell.execute_reply.started":"2022-11-20T19:59:14.090533Z"},"id":"jG7aZ4ahkCDL","outputId":"32e2811f-f80b-4f89-a93b-d2d828fd80af","trusted":true},"outputs":[],"source":["# CNN-static: pretrained word vectors are used and freezed during training.\n","# set_seed(42)\n","cnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n","                                        freeze_embedding=True,\n","                                        learning_rate=0.5,\n","                                        dropout=0.5,\n","                                        num_classes=8,\n","                                        weight_decay=1e-3)\n","count_params(cnn_static)\n","acc = []\n","train_t = []\n","for i in range(10):\n","    best_acc, train_time = train(cnn_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"r8_cnn_static\")\n","    acc.append(best_acc)\n","    train_t.append(train_time)\n","\n","# cal avg and std of acc and time\n","acc_mean, acc_std = mean_std(acc)\n","t_mean, t_std = mean_std(train_t)\n","print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n","print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEA8uht7kCIX"},"outputs":[],"source":["# CNN-non-static: pretrained word vectors are fine-tuned during training.\n","# set_seed(42)\n","cnn_non_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n","                                            freeze_embedding=False,\n","                                            learning_rate=0.5,\n","                                            dropout=0.5,\n","                                            num_classes=8,\n","                                            weight_decay=1e-3)\n","count_params(cnn_non_static)\n","acc = []\n","train_t = []\n","for i in range(10):\n","    best_acc, train_time = train(cnn_non_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"r8_cnn_non_static\")\n","    acc.append(best_acc)\n","    train_t.append(train_time)\n","    \n","# cal avg and std of acc and time\n","acc_mean, acc_std = mean_std(acc)\n","t_mean, t_std = mean_std(train_t)\n","print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n","print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"]},{"cell_type":"markdown","metadata":{"id":"uAQYAze5paRB"},"source":["## Test "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAZ-pBx-pdbo"},"outputs":[],"source":["def predict_review(text, model=cnn_non_static.to(\"cpu\"), max_len=62):\n","    \"\"\"Predict probability that a review is positive.\"\"\"\n","\n","    # Tokenize, pad and encode text\n","    tokens = word_tokenize(text.lower())\n","    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n","    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n","\n","    # Convert to PyTorch tensors\n","    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n","\n","    # Compute logits\n","    logits = model.forward(input_id)\n","\n","    #  Compute probability\n","    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n","\n","    print(f\"This review is {probs[1] * 100:.2f}% positive.\")\n","\n","r8_id2cat = {\n","    \"acq\": 0,\n","    \"crude\": 1,\t\n","    \"earn\": 2,\t\n","    \"grain\": 3,\n","    \"interest\": 4,\t\n","    \"money-fx\": 5,\n","    \"ship\": 6,\n","    \"trade\": 7\n","}\n","\n","def predict_r8(text, model=cnn_non_static.to(\"cpu\"), max_len=62):\n","    \"\"\"Predict probability of each category in r8.\"\"\"\n","\n","    # Tokenize, pad and encode text\n","    tokens = word_tokenize(text.lower())\n","    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n","    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n","\n","    # Convert to PyTorch tensors\n","    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n","\n","    # Compute logits\n","    logits = model.forward(input_id)\n","\n","    #  Compute probability\n","    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n","\n","    for i in range(8):\n","        print(f\"This review is {probs[i] * 100:.2f}% {r8_id2cat[i]}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fr6SDbqYq2Hh"},"outputs":[],"source":["test_model = torch.load(\"./models/mr_cnn_non_static_best_model.pt\")\n","test_model.to(\"cpu\")\n","test_model.eval()\n","predict_review(\"All of friends slept while watching this movie. But I really enjoyed it.\", model=test_model)\n","predict_review(\"I have waited so long for this movie. I am now so satisfied and happy.\", model=test_model)\n","predict_review(\"This movie is long and boring.\", model=test_model)\n","predict_review(\"I don't like the ending.\", model=test_model)"]},{"cell_type":"markdown","metadata":{},"source":["### Imapct of Document Length(R8)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T20:00:37.300124Z","iopub.status.busy":"2022-11-20T20:00:37.299403Z","iopub.status.idle":"2022-11-20T20:01:27.491900Z","shell.execute_reply":"2022-11-20T20:01:27.490775Z","shell.execute_reply.started":"2022-11-20T20:00:37.300085Z"},"trusted":true},"outputs":[],"source":["def len_text(input_id):\n","    cnt = 0\n","    for token in input_id:\n","        if token != 0:\n","            cnt += 1\n","    return cnt\n","\n","def evaluate_doc_length(model, test_dataloader):\n","    \"\"\"\n","    we divide documents into extreme short (less than 30 words), \n","    short(30-50 words), medium (50-70 words), and long (more than 70 words)..\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled\n","    # during the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    c_es = 0\n","    c_s = 0\n","    c_m = 0\n","    c_l = 0\n","    t_es = 0\n","    t_s = 0\n","    t_m = 0\n","    t_l = 0\n","\n","    # For each batch in our validation set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n","        \n","        for input_id, label in zip(b_input_ids, b_labels):\n","\n","            # Compute logits\n","            length = len_text(input_id)\n","            with torch.no_grad():\n","                input_id = input_id.clone().detach().unsqueeze(dim=0)\n","                logit = model(input_id)\n","\n","            # Get the predictions\n","            pred = torch.argmax(logit)\n","\n","            # Calculate the accuracy rate\n","            if length > 70:\n","                t_l += 1\n","                if pred == label:\n","                    c_l += 1\n","            elif length >= 50:\n","                t_m += 1\n","                if pred == label:\n","                    c_m += 1\n","            elif length >= 30:\n","                t_s += 1\n","                if pred == label:\n","                    c_s += 1\n","            else:\n","                t_es += 1\n","                if pred == label:\n","                    c_es += 1\n","                \n","\n","    # Compute acc\n","    print(\"Acc of extreme short: \" + str(c_es/t_es))\n","    print(\"Acc of short: \" + str(c_s/t_s))\n","    print(\"Acc of medium: \" + str(c_m/t_m))\n","    print(\"Acc of long: \" + str(c_l/t_l))\n","    \n","evaluate_doc_length(cnn_static, test_dataloader)\n","    "]},{"cell_type":"markdown","metadata":{"id":"U25M9LGrbl9e"},"source":["## Conclusion\n","* CNN networks looks effective on text classification problem, it's competitive to other networks like LSTM, GNN, etc.\n","* We reached around 77-78% testing accuracy on Movie Review dataset, around 97% testing accuracy on R8 dataset, with a really fast training speed\n","* We tried with several hyperparameters but we did not conduct any hyperparameters search, however, the accuracy looks good\n","* Glove pretrained embeddings definitely helps, but the non-static embeddings doesn't improve much"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":4}
